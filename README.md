# Recommendation Pipeline for E-commerce

A production-grade, event-driven platform for ingesting, processing, and serving recommendations at scale.

---

## üéØ Project Goals
1. **High-throughput ingestion** of click-stream and catalog updates via Apache Kafka.  
2. **Stream and batch feature computation** for real-time and offline models.  
3. **Modular model registry and serving** so any team can plug in a recommender.  
4. **Observability by design**‚Äîmetrics, traces, and structured logs out of the box.

---

## üó∫Ô∏è Roadmap

| Stage | Status | High-level Deliverables |
|-------|--------|-------------------------|
| **0 ‚Äì Bootstrap** | ‚úÖ *(this commit)* | ‚Ä¢ Repository skeleton<br>‚Ä¢ Documentation foundations |
| **1 ‚Äì Messaging Backbone & Event Simulation** | ‚¨ú | ‚Ä¢ Local single-node Kafka via Docker Compose<br>‚Ä¢ Event generator script producing realistic product-interaction events |
| **2 ‚Äì Feature Engineering** | ‚¨ú | ‚Ä¢ Stream processors (Kafka Streams / Flink)<br>‚Ä¢ Feature store PoC |
| **3 ‚Äì Model Training & Registry** | ‚¨ú | ‚Ä¢ Offline pipelines (Spark/ Pandas)<br>‚Ä¢ MLflow registry integration |
| **4 ‚Äì Real-time Serving** | ‚¨ú | ‚Ä¢ gRPC/REST recommendation service<br>‚Ä¢ A/B experiment hooks |
| **5 ‚Äì Observability & CI/CD** | ‚¨ú | ‚Ä¢ Grafana dashboards<br>‚Ä¢ GitHub Actions for build/test/deploy |

> **Tip:** Each stage lives in its own branch until it is production-ready, then merges to **main** via pull request.

---

## üì¶ Stage 1 in Detail

### Objective
Stand up a reliable messaging backbone (**Apache Kafka**) and produce a continuous stream of synthetic events so downstream components have data from day one.

### Task-by-task Breakdown
| # | Task | Success criteria |
|---|------|------------------|
| 1 | **Spin up Kafka & Zookeeper** using Docker Compose | `docker compose ps` shows healthy `kafka` and `zookeeper` containers |
| 2 | **Create bootstrap topics** (`product-clicks`, `cart-adds`, `purchases`, etc.) | `kafka-topics --list` prints all required topics |
| 3 | **Write an event-generator CLI** (`scripts/event_generator.py`) | ‚Ä¢ Configurable throughput (events/sec)<br>‚Ä¢ JSON payloads validated against Avro/JSON Schema<br>‚Ä¢ Sends to correct topic |
| 4 | **Add Make targets & README usage section** | `make up` brings the stack up; `make simulate` starts the producer |
| 5 | **Smoke-test consumer** (`scripts/print_events.py`) | Events printed to stdout with correct schema |
| 6 | **CI gate: Stage-1 test job** | GitHub Actions workflow passes on every PR |

### Local Quick-start (once Stage 1 is done)

```bash
# Bring everything up
docker compose up -d

# Generate 500 events at ~50 events/s
python scripts/event_generator.py --count 500 --rate 50

# Peek at the stream
python scripts/print_events.py --topic product-clicks

